\fakepar{Step 1:\ Adopt a Prima-facie Duty Theory} Prima-facie is a statement that is considered correct until proven otherwise.\ It provides two advantages: duties can be updated if proven incorrect, and the freedom to choose the domain-specific duties subject to the assumption that there is a common consensus among the domain experts to treat them as correct.\ Anderson et al.\ chose the biomedical domain whose ethics rely on principles given by Beauchamp and Childress - autonomy, nonmaleficence, beneficence, and justice \cite{beauchamp2001principles}.

\fakepar{Step 2:\ Conflicting subset of the duties} In the medical domain, there is a classic example of conflict between three biomedical ethical principles, namely:\ autonomy, nonmaleficence, and beneficence.\ E.g., a health care professional has recommended a particular treatment for their competent adult patient, and the patient has rejected that treatment option.\ Should the health care professional try again to change the patient’s mind?\ Anderson et al.\ \cite{Anderson_Anderson_2007}, through this example, convey that we need to find the inconsistent cases where these duties will conflict.\ We need to categorize previous and new cases in a particular subset of these duties where all the elements of that subset are relevant and might conflict.

\fakepar{Step 3:\ Selection of range of satisfaction and violation levels} Anderson et al.\ \cite{Anderson_Anderson_2007} state that adopting a prima-facie duty approach often lacks decision making when the duties conflict.\ Thus, we need to balance the levels of satisfaction (a positive integer) and violation (a negative integer) of these duties and design an algorithm that takes such case profiles and outputs the action consistent with these duties.\ A profile of an ethical dilemma is an ordered set of numbers for each possible action that can be performed where the numbers reflect the duties being satisfied or violated, see Table~\ref{profile}.\ The checkmark on ``try again" indicates the correct action, i.e., the action with the highest score.\ 

\begin{table}[h]
    \caption{MedEthEx training case 1}
    \label{profile}
    \scriptsize
    \centering
    \begin{tabular}{|p{2.5cm}|p{0.75cm}|p{0.75cm}|p{0.75cm}|}
        \hline
         \textbf{Training case} 1 & \textbf{A} & \textbf{N} & \textbf{B}\\
         \hline
         $\checkmark$ \textbf{Try again} & -1 & +2 & +2 \\
         \hline
         \textbf{Accept} & +1 & -2 &-2 \\
         \hline
         \multicolumn{4}{|p{7cm}|}{A=autonomy,\ N=nonmaleficence,\ B=beneficence}\\
         \hline
    \end{tabular}
\end{table}

Such profiles can abstract a principle using a learning algorithm that can be tested on new cases for its further refinement.\ There are two advantages to this approach:\ possibility to change the range of levels and add new duties as needed. \nowidow[]

\fakepar{Step 4:\ Algorithmic design} Implementing the algorithm requires the formulation of a principle to determine the correct action when the duties conflict.\ Anderson et al.\ used inductive logic programming (ILP) from traditional machine learning to abstract relationships between the prima-facie duties.\ ILP is concerned with inductively learning relations represented as first-order Horn clauses (that is, universally quantified conjunctions of positive literals $L_i$ implying a positive literal $H: H <= (L_1 ∧ … ∧ L_n)$).\ ILP is used to learn the relation supersedes(A1, A2), which states that action A1 is preferred over action A2 in an ethical dilemma involving these choices.\ Actions are represented as ordered sets of integer values in the range of +2 to –2, provided by the user, where each value denotes the satisfaction (positive values) or violation (negative values) of each duty involved in that action, see Table~\ref{profile}.\ Clauses in the supersedes predicate are represented as disjunctions of lower bounds for differentials of these values.\ ILP was chosen to learn this relation for three reasons.\ First, the non-classical relationships that might exist between duties can be expressible in the representation language provided by ILP.\ Second, the consistency of a hypothesis regarding the relationships between duties can be confirmed across all cases when represented as Horn clauses.\ Finally, commonsense background knowledge regarding the supersedes relationship can be expressed and consulted in ILP’s declarative representation language.\\ 
\indent The objective of the training is to learn a complete and consistent hypothesis.\ A positive example is the one where the first action supersedes the second and a negative example is the opposite (by inverting the order of these actions).\ A complete hypothesis covers all positive cases, and a consistent one covers no negative cases.\ The system starts with a general hypothesis stating that all actions supersede each other and covers all positive and negative cases.\ The system is then provided with positive cases (and their negatives) and modifies its hypothesis by adding or refining clauses, such that it covers given positive cases and does not cover given negative cases.\ Once the principle was discovered, given a new profile representing respective satisfaction and violation levels of the duties involved in each possible action, values of corresponding duties are subtracted (those of the second action from those of the first).\ The principle is then consulted to see if the resulting differentials satisfy any of its clauses.\ If so, the first action of the profile is deemed ethically preferable to the second. 

\fakepar{Step 5:\ Validation through advisory agents} Anderson et al.\ \cite{Anderson_Anderson_2007} explored two prototype applications in a form of advisory agents.\ First, MedEthEx is an expert medical system that uses the discovered principle and decision procedure to advise a user faced with a case previously described.\ To permit the use by someone unfamiliar, a user interface was developed that (1) asks ethically relevant questions to the user regarding the particular case at hand, (2) transforms the answers to these questions into the appropriate profiles, (3) sends these profiles to the decision procedure, (4) presents the answer provided by the decision procedure, and (5) provides a justification for the answer\footnote{An implementation is available at \url{https://www.machineethics.com/}}. The decision principle, in above-mentioned healthcare dilemma, that the MedEthEl system discovered was: a health care worker should challenge a patient’s decision if it is not fully autonomous and there is either any violation of nonmaleficence or a severe violation of beneficence.\\\
\indent Second, EthEl is a reminder system for eldercare to take medication and to contact an overseer if they refuse the reminder.\ It receives input from an overseer (a doctor), including the prescribed time to take a medication, the maximum amount of harm that could occur if this medication is not taken (e.g., none, some, or considerable), the number of hours it would take for this maximum harm to occur, the maximum amount of expected good by taking this medication, and the number of hours it would take for this benefit to be lost.\ The system determines from this input the change in duty satisfaction and violation levels over time, a function of the above-mentioned inputs from the overseer.\ This value is used to increment duty satisfaction and violation levels for the remind action and, when a patient disregards a reminder, the notify action.\ A reminder is issued when the levels of satisfaction or violation have reached the point where reminding is ethically preferable to not reminding.\ Similarly, the overseer is notified when a patient has disregarded the reminders and the duty levels have reached the point where notifying the overseer is ethically preferable to not notifying.

\fakepar{Step 6:\ Evaluation} Anderson et al.\ \cite{Anderson_Anderson_2007} implemented a variant of the test Alan Turing \cite{10.2307/2251299} suggested as a means to determine intelligence of a machine that bypassed disagreements concerning definitions of ethical behaviour and the machine's ability to articulate its decisions, namely, ``Comparative Moral Turing Test" (cMTT).\ An evaluator assesses the comparative morality of pairs of behaviour where one describes the actions of a human being and that of a machine faced with the same ethical dilemma.\ If the machine is not identified as the less moral member of the pair significantly more often than the human, then it has passed the test.\ They point out, though, that human behaviour is typically far from being morally ideal and a machine that passed the cMTT might still fall far below these moral standards.\ This concern suggests that the comparison be made with behaviour recommended by a trained ethicist faced with the same dilemma.\ Also the principles used to justify the decisions that are reached by both the machine and ethicist should be made transparent and compared.